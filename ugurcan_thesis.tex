\documentclass[12pt,oneandhalf,chaparabic,ceng,ms,eng,oneside,pntc]{gsufbe}
% for Computer Engineering use option ceng
% for Industrial Engineeering use option ie
% for Logistics and Finance Management use option lfm
% for Mathematics use option math
% for MSc use option ms
% for PhD use option phd
% Don't change other options due to instutional regulations. 
% You can delete next line If your thesis does not have an appendix
\usepackage{appendix}

% Use your latex packages here
%\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{array}
\usepackage[round]{natbib}
\usepackage{har2nat}
\usepackage{algorithmic}
\usepackage[ruled,noline]{algorithm2e}
\usepackage{titlesec}
\usepackage[section]{placeins}
\usepackage{float}

% End of Latex Packages
%
% Any personal Latex definition, decleration, etc.
\makeatletter
\let\old@includegraphics\includegraphics
\renewcommand{\includegraphics}[2][,]{%
  \setbox9=\hbox{\old@includegraphics[#1]{#2}}%
  \ifdim\wd9>\textwidth
    \old@includegraphics[#1,width=\textwidth]{#2}%
  \else
    \old@includegraphics[#1]{#2}%
  \fi%
}
\makeatother

% End of personal stuff
%
% Personal Information
% ----------------------------
%
% Please check this part and fill in information about your thesis
%
% Name and Surname
\author{Uğurcan Ergün}
% Thesis Title English and Turkish
\title{Evaluating Feasibility of Container Virtualization for Network Function Virtualization}
\trtitle{Ağ İşlevi Sanallaştırma için Konteynır Sanallaştırmanın Uygunluğunun İncelenmesi}
% Department : English and Turkish
%
% The departments are pre-defined, you need not redeclare them. 
% Date : You should indicate the month of your thesis defence in English.
% Default is this month
%
\date{June 2018}
%
% Approval Page Details
% --------------------------
% For each command you can give the title as optional parameter enclosed in [ ]
%For white covered thesis, please comment out approval page in cls file.
%Committee members number:
%-------------------
%Single advisor/masters thesis: 2 members excluding advisor
%Two advisors/master thesis: 3 members excluding both advisors
%Single advisor/PhD Thesis: 4 members excluding advisor
%Two advisors/PhD Thesis: 5 members excluding advisor
%Please comment out extra member of committee. 
%
% prof : Prof. Dr.
% assocprof : Assoc. Prof. Dr.
% assistprof : Assist. Prof. Dr.
% dr : Dr.
%
% Supervisor
\supervisor[assistprof]{B. ATAY ÖZGÖVDE}
\departmentofsupervisor{Computer Engineering Department, GSU}
% co-Supervisor

%\cosupervisor[assocprof]{JANE DOE}
%\departmentofcosupervisor{Management, UCL}
% Ask your supervisor if you are not sure
\committeememberi[assistprof]{}
\affiliationi{}
\committeememberii[assistprof]{}
\affiliationii{}
%\committeememberiii[assistprof]{ATAY ÖZGÖVDE}
%\affiliationiii{Computer Engineering Department, GSU}
% Fourth committee member
%\committeememberiv[assocprof]{MAN DOE}
%\affiliationiv{Computer Engineering Department, MIT}
% Fifth committee member
%\committeememberv[assistprof]{WOMAN DOE}
%\affiliationv{Computer Engineering Department, TOBB ETÜ}
%
% Keywords : English & Turkish & French, Comma seperated No more than 5 keywords
\keywords{}
\motscles{}
\anahtarklm{}
%
% Abstract in English
%
\abstract{}
%
% Abstract in French
\resume{}
% Turkish Abstract
%
\oz{}
%
% Acknowledgements
\acknowledgments{}
%
% End of Personal and Introductory Information
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\setlength{\jot}{20pt}
%%% !!! This two should be last lines before \begin{document}, do no move them !!!
\usepackage[pdftex]{hyperref}
\usepackage[all]{hypcap}
\begin{document}
\addtolength{\textheight}{1.5cm}
% Preliminaries
\newlength\myindent
\setlength\myindent{6em}
\newcommand\bindent{%
  \begingroup
  \setlength{\itemindent}{\myindent}
  \addtolength{\algorithmicindent}{\myindent}
}
\newcommand\eindent{\endgroup}
\begin{preliminaries}
% If you are willing to use any custom stuff before Chapters, put it here
% Such as List of Abbreviations
% Check the abbreviations.tex for a template list of abbreviations
%\input{abbreviations.tex}
% End of Preliminaries
\end{preliminaries}
%
% Latex content Goes Here
%
%
\newtheorem{thm}{Definition}[chapter]
\renewcommand{\thethm}{\arabic{chapter}.\arabic{thm}}
\newtheorem{prp}{Proposition}[chapter]
\renewcommand{\theprp}{\arabic{chapter}.\arabic{prp}}
\newenvironment{prf}{\noindent{\bf Proof}}{$\hfill \Box$ \vspace{10pt}}

\chapter{Introduction}

\chapter{Literature Review}

\chapter{Network Function Virtualization}
\section{Introduction} 
\section{ETSI MANO}
\subsection{Architecture}
\subsection{Implementations}
\section{Service Function Chaining}
\chapter{Container Virtualization}
\section{Introduction}
National Institute of Standards and Technology defines cloud computing as a model for enabling
easy access to a pool of computing resources that requires minimal intervention from the service
provider. With cloud services, any person or organization can buy computing resources from providers
without the need of investing in computing infrastructure themselves. 

One of the essential technologies for implementing cloud services is virtualization. In modern cloud
environments the usually preferred method for virtualization is called hypervisor based virtualization.
Xen and KVM can be called the two most common free and open source hypervisors. While Xen is used in
biggest public cloud platform Amazon Web Services, KVM is used in Google Cloud Platform and Openstack.

The main goal for the hypervisors is to isolate virtual machines in physical host to a degree that they
are no different than separate computers in a network. Virtual machines run a separate operating system
called guest operating system and hypervisor mediates the system calls to the host operating system.
This technique is very efficient at isolating virtual machines but it also introduces an overhead
since there is an extra layer of abstraction. Previous studies show that while CPU and memory overhead
is minimal, biggest overheads occur in I/O operations. There is also some operational overheads that
comes from running an separate full-fledged guest OS. It also needs to be maintained and updated.

Recently there is another method of virtualization gaining traction in both research and the industry.
It's called operating system level virtualization or more commonly container virtualization.With
container virtualization virtual machines or containers have their own process and resource space but
they use host machines kernel and resources directly. Without the need of complete isolation and an
unaware guest OS container virtualization can achieve faster start up times, less resource consumption
and smaller images. But since all containers use the same kernel isolation between different containers
is weaker.

The earliest attempt for operating system level virtualization can be traced to the UNIX chmod command.
It is introduced in UNIX version 7 at year of 1979. By that time it could only provide file system
isolation. But FreeBSD jails is the first mechanism that can truly be called a container in a modern
sense. It was developed by Poul-Henning Kamp at 1998 and introduced in FreeBSD version 4 at 2000.
Jailed processed can't see other processes, can access only a specific part of their file systems and
have their own IP addresses. At Linux side, OpenVZ project developed a specialized kernel for running
containers at 2006. Contrary to jails, OpenVZ is more akin to a virtual machines. OpenVZ containers
have their own user structure, subject to configurable resource limits and can be migrated to separate
physical machines without the need of shutting down.

Despite both the concept and the technologies that make it possible were around for some time.
Containers are only recently started to see wide spread adoption. This can be explain with a couple
reasons. While Linux became one of the major platforms in the server domain there wasn't any container
technology available that can be run on an unmodified Linux kernel. Furthermore Linux kernel lacked
some of the essential capabilities to implement a container system such as resource management for
processes. Before cgroups was introduced in 2008 Linux kernel didn't have the capacity for controlling
and limiting the resources of a process or group of processes. Using cgroups and earlier introduced
kernel namespaces in the same year first native Linux
container project LXC was announced. While it didn't see much adopted by itself. It formed the basis
for the most of the modern container technologies.

\section{Docker}
Docker is a container virtualization technology that is developed in 2013 by a software company called
dotCloud. Among the modern container technologies Docker is the most widely used one and it would also
be fair to say Docker project's success is partially responsible for the recent adoption of container
virtualization technology. This might be attributed to Docker's different approach on how to prepare 
and use containers.

Virtual machines are used to be considered as virtual equivalents of physical dedicated servers and for
some time containers are also thought as faster and more lightweight virtual machines. But the main
contribution of Docker is focusing into an application instead of a server. Docker provides tools for
packaging an application and all of its dependencies together to run the application inside of a
container. This model of packaging and deployment of applications using containers also found out to be
a effective solution for an important problem at software development and maintenance. Because 
differences between development and production environments such as different operating systems, 
different versions of libraries and applications etc. is a common source for software bugs. Since they
are being used and  maintained by different people and there are no way any single person would be
aware of all the differences. Packaging the application and dependencies with Docker and running it in
containers prevents this problem. Because it guarantees the environment that runs in developer’s
machine and the production machine will be the same.

Furthermore Docker proposes that each component of an application should be run in a 
different container thus encouraging developers to use a loosely coupled software architecture that is 
also called micro service architecture. According to micro service architectures managing and updating
individual components of applications are easier compared to managing a monolithic application which
all of its components runs in same environments and problems in one component can affect the whole
application.

\subsection{Docker Images}
As mentioned before packaging the applications and dependencies together is one of the core ideas
behind Docker. This is achieved by using the Docker images. Contrary to virtual machines where user
has to install a new operating system on a clean virtual disk or use pre-installed disk image. Docker 
provides the tools to easily create new images from scratch or from existing images. Docker also
provides a central registry called Docker Hub where official and community created images can be found.
New images are created by using a template format called Dockerfiles.To create an image user has to
choose a base image and then has to describe what changes has to be made on this base image with
provided commands. An example Dockerfile can be found at the appendix. Then using the Docker build
command will create the intended image.

Docker images are declarative constructs. An existing Docker image can not be edited or changed. This
way it's guaranteed to have the same environment every time an image is used. To make changes to an
image a new image has to be created. Docker has a version control system for images and a new version
of the image doesn't have to overwrite the old images. This way in case of errors and failures old
versions can still be used. Since storing whole copies of all images would take too much disk space.
Docker uses a different storage technologies for storing images.

Docker images are stored using a storage model called union file systems to minimize storing duplicate
data. Using this model images are consisted of several ordered layers. Each layer has it's own file
system and if a file is present in multiple layers only the file on the top most layer is visible. When
creating a new image Docker takes all the layers from the base image and adds new layer or layers
according to the Dockerfile. If we make create a new version of a image only the changes are also 
written to a new layer. Normally layers are read only file systems but since container that can not
write data to their disks would be unusable a writable layer is added when a container is created. This
layer would be deleted when the container is removed until it's saved as a version of the image.
While Docker can use different union file systems, by default AUFS is used.

\subsection{Architecture}
Docker uses a rather simple server-client architecture for the creation and management of the
containers. The main components can be summarized as the Docker engine, the component that manages
images and containers, Docker client, the component that users and administrators can send their
commands to Docker engine with and the Docker registry the component images that constitutes the core of
the containers resides. The communication between these components are achieved via REST APIs.

It's clear to see that the Docker engine is at the core of this architecture. It will be easier to
understand how the Docker Engine currently works if it's broken down to three sub components. The
container runtime is the component that runs the containers themselves. The container engine provides
the extra services that a container will need such as networking, storage, security etc. And The Docker 
services is the set of components that provides services that are crucial for Docker's internal
operation such as the Docker API, image management, authentication etc.

Docker's commitment to the above mentioned micro service architecture can also be seen in the evolution
of the Docker engine as it matures over time. Initially it was just a monolithic daemon. While it's
convenient to have a centralized system in the short run. In time the shortcomings of this model
started to appear and components started to become independent from the daemon. At first a plug-in
model developed to manage the aforementioned extra services. Then the container runtime was modified to
be able to run in a stand alone way. While the first container runtime for Docker was the LXC. It was
already replaced by a runtime called libcontainer which is developed by Docker themselves. The new
runtime that enabled libcontainer to be used without the docker daemon is called runC and it's managed
by a library called containerd. With this change docker daemon became a component that is only
connecting the API, service plugins and the containerd.

\section{Kubernetes}

\chapter{Experimentation}
\section{Setup}
This section will explain the test environments that all the experimentations will use. While this
setup has many components, all the software components are selected from available free and open source
tools.

The hardware used in this setup is a desktop computer. It's specifications are in the following
\begin{itemize}
 \item 8 core Intel i7-920 processor
 \item Asus P6T Motherboard
 \item 20 GiB 1066 Mhz DDR3 memory
 \item Ubuntu 16.04 64 bit operating system
\end{itemize}

One of most essential parts of this setup would be a cloud platform that can run and manage containers.
We chose the Kubernetes container orchestration software as a cloud platform after considering it's
features, widespread adoption and maturity. There is no standard way to setup a Kubernetes cluster.
While minikube can be used for development and testing purposes. It's method of creating a single node
cluster inside of a virtual machine would makes it unsuitable for mimicking actual clusters. Another
tool for building clusters by the Kubernetes team is kubeadm. Although it's currently on beta, it
allows easy installation among multiple nodes regardless of type. We selected kubeadm as the 
installation tool for our work. Installing kubeadm inside of Docker containers instead of virtual
machines we built a three machine Kubernetes cluster with one master and two nodes. This pattern is
often called dind or Docker in Docker. Kubeadm also leaves the network setup to the administrator's
choice via the CNI. Thus allowing us to experiment with different network configurations.

For monitoring the cluster state and container performance, an external tool is needed. As a monitoring
tool we chose Prometheus a modern pull based monitoring that is developed by the same non-profit that
also develops Kubernetes. Prometheus can be easily installed using the operator model. It can also
receive metrics from other applications via extensions called exporters. To use in the tests we also 
prepared a Docker image with a modified web server to be able send usage metrics to Prometheus. This
docker image contains a Nginx web server version 1.12.2 compiled with nginx-vts module.

\section{Experiment Design}
Before we start with experimenting with more advanced applications, it would be preferable to see this
setup's performance when a more typical application is used. We designed a preliminary experiment for
measuring network performance on Kubernetes using several different network setups.

As we mentioned before contrary to virtual machines, if they are not constrained containers have full
access to host machines computing resources and in a typical cloud environment we can assume that
containers will always run with constrained resources. Because of that will also constrain our
containers' resources. Until otherwise stated each pod will be constrained to 1/10th of a CPU core.

The test application will be a simple web server. It will be deployed to the cluster via Kubernetes
deployments using our Docker image. Kubernetes deployments are ideal deployment tools for this case 
because they can create and maintain a specified number of instances for an application and they also
can easily be scaled horizontally. We will create 3 separate deployments and for varying number of
instances in each deployment we will run series of tests simulating network traffic. For simulating
network traffic a http load testing tool bombardier will be used. It's a command line tool written in
Go programming language and it can save results in JSON format for easy further processing. All
containers inside a deployment will be exposed through single endpoints using Kubernetes NodePort 
services.

Our methodology for running the tests will be the following. 
\begin{enumerate}
 \item First we will choose a network setup and a node count and then create a Kubernetes cluster.
 \item For each step we will create a pre-determined amount of instances for each deployment.
 \item For each deployment we will run the load test software to send various amount of requests from
 various amount of concurrent connections
\end{enumerate}

Number of instances used in the tests and parameters used for the load testing tools can be found below.

\begin{table}[h]
\caption{Number of instances for each deployment}
\centering
\begin{tabular}{ccc}
Deployment 1 & Deployment 2 & Deployment 3 \\
\specialrule{2pt}{1pt}{1pt}
1 & 1 & 1 \\
5 & 5 & 1 \\
10 & 1 & 1 \\
12 & 12 & 1 \\
25 & 1 & 1 \\
25 & 25 & 1 \\
37 & 37 & 1 \\
50 & 1 & 1 \\
50 & 50 & 1 \\
75 & 1 & 1 \\
100 & 1 & 1 \\
\hline
\end{tabular}
\label{rn1}
\end{table}

\begin{table}[h]
\caption{Web server load testing parameters}
\centering
\begin{tabular}{cc}
Connections & Total Requests\\
\specialrule{2pt}{1pt}{1pt}
20 & 10000 \\
40 & 30000 \\
60 & 50000 \\
80 & 80000 \\
100 & 100000 \\
\hline
\end{tabular}
\label{rn1}
\end{table}

\section{Results}

\chapter{Conclusion}

\appendix
\thispagestyle{empty}
%\chapter[]{Proof of Some Theorem}
%\thispagestyle{empty}
%This is appendix text.

%\input{vita.tex}
%\thispagestyle{empty}

\end{document} 